{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from alphazero import PolicyValueNet\n",
    "from alphazero.train import PolicyValueLoss\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b7a90774285caef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GameDataset(Dataset):\n",
    "    \"\"\" 自我博弈数据集类，每个样本为元组 `(feature_planes, pi, z)` \"\"\"\n",
    "\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self.__data_deque = deque(data_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__data_deque)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__data_deque[index]\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" 清空数据集 \"\"\"\n",
    "        self.__data_deque.clear()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afab733ea620fa76",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "# 创建数网络\n",
    "# policy_value_net = PolicyValueNet(board_len=7, n_feature_planes=13, policy_output_dim=100, is_use_gpu=True)\n",
    "policy_value_net: PolicyValueNet = torch.load(\"./model/policy_value_net_100.pth\")\n",
    "\n",
    "# 创建优化器和损失函数\n",
    "optimizer = Adam(policy_value_net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = PolicyValueLoss()\n",
    "\n",
    "# self.lr_scheduler = MultiStepLR(self.optimizer, [1500, 2500], gamma=0.1)\n",
    "# lr_scheduler = ExponentialLR(optimizer, gamma=0.998)  # 0.998 ** 1000 = 0.135\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7035d53ca9a74002",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92711a21935dafdb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_list_ = torch.load(\"./data/maxrand_data_650k.pth\")\n",
    "data_list = []\n",
    "for data in tqdm(data_list_, ncols=80, desc=\"Loading data to GPU\"):\n",
    "    f, p, zi = data\n",
    "    f, p, zi = f.to(device), p.to(device).float(), zi.to(device).float()\n",
    "    data_list.append((f, p, zi))\n",
    "\n",
    "data_list_ = None\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b34bc37741a8fd1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = GameDataset(data_list)\n",
    "print(len(dataset))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05521f6c8a1826a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e88d96bd55f0c3e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "policy_value_net.train()\n",
    "loss_history = []\n",
    "\n",
    "epoch_num = 500\n",
    "save_freq = 100\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    p_bar = tqdm(enumerate(data_loader, 0), ncols=80, total=len(data_loader), desc=f\"Epoch {epoch + 1}\")\n",
    "    for i, data in p_bar:\n",
    "        feature_planes, pi, z = data\n",
    "\n",
    "        # 前馈\n",
    "        p_hat, value = policy_value_net(feature_planes)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算损失\n",
    "        # loss = criterion(p_hat.float(), pi.float(), value.flatten().float(), z.float())\n",
    "        loss = criterion(p_hat, pi, value.flatten(), z)\n",
    "        # 误差反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        # 学习率退火\n",
    "        # lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {loss.item():.4f}\")\n",
    "    loss_history.append(loss.item())\n",
    "    if (epoch + 1) % save_freq == 0:\n",
    "        torch.save(policy_value_net, f\"./model/policy_value_net_{epoch + 1}.pth\")\n",
    "        print(f\"Save model to ./model/policy_value_net_{epoch + 1}.pth\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ac4dbef6a8afd789",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7a138860cd93ea2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(policy_value_net, f\"./model/policy_value_net_175.pth\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e08b125303b5489f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cbaf1539e90e4003"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
